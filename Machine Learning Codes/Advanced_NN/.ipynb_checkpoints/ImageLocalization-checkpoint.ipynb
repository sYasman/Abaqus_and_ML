{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6Pnix2iKXR5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpiJj8ym0v0-"
   },
   "source": [
    "<a name=\"0-5\"></a>\n",
    "## 0.5 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBCQ8jnFygYk"
   },
   "outputs": [],
   "source": [
    "# Install this package to use Colab's GPU for training\n",
    "!apt install --allow-change-held-packages libcudnn8=8.4.1.50-1+cuda11.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AoilhmYe1b5t"
   },
   "outputs": [],
   "source": [
    "import os, re, time, json\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddcggt0WKXR5"
   },
   "source": [
    "Store the path to the data.\n",
    "- Remember to follow the steps to `set up the data location` (above) so that you'll have a shortcut to the data in your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upfxqqK0vTMc"
   },
   "outputs": [],
   "source": [
    "data_dir = \"/content/drive/My Drive/TF3 C3 W1 Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmoFKEd98MP3"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1. Visualization Utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOhS3mNlDOLX"
   },
   "source": [
    "<a name=\"1-1\"></a>\n",
    "### 1.1 Bounding Boxes Utilities\n",
    "\n",
    "\n",
    "- `draw_bounding_box_on_image`: Draws a single bounding box on an image.\n",
    "- `draw_bounding_boxes_on_image`: Draws multiple bounding boxes on an image.\n",
    "- `draw_bounding_boxes_on_image_array`: Draws multiple bounding boxes on an array of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWIHFPa0uOC_"
   },
   "outputs": [],
   "source": [
    "def draw_bounding_box_on_image(image, ymin, xmin, ymax, xmax, color=(255, 0, 0), thickness=5):\n",
    "    \"\"\"\n",
    "    Adds a bounding box to an image.\n",
    "    Bounding box coordinates can be specified in either absolute (pixel) or\n",
    "    normalized coordinates by setting the use_normalized_coordinates argument.\n",
    "    \n",
    "    Args:\n",
    "      image: a PIL.Image object.\n",
    "      ymin: ymin of bounding box.\n",
    "      xmin: xmin of bounding box.\n",
    "      ymax: ymax of bounding box.\n",
    "      xmax: xmax of bounding box.\n",
    "      color: color to draw bounding box. Default is red.\n",
    "      thickness: line thickness. Default value is 4.\n",
    "    \"\"\"\n",
    "  \n",
    "    image_width = image.shape[1]\n",
    "    image_height = image.shape[0]\n",
    "    cv2.rectangle(image, (int(xmin), int(ymin)), (int(xmax), int(ymax)), color, thickness)\n",
    "\n",
    "\n",
    "def draw_bounding_boxes_on_image(image, boxes, color=[], thickness=5):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes on image.\n",
    "    \n",
    "    Args:\n",
    "      image: a PIL.Image object.\n",
    "      boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
    "             The coordinates are in normalized format between [0, 1].\n",
    "      color: color to draw bounding box. Default is red.\n",
    "      thickness: line thickness. Default value is 4.\n",
    "                           \n",
    "    Raises:\n",
    "      ValueError: if boxes is not a [N, 4] array\n",
    "    \"\"\"\n",
    "    \n",
    "    boxes_shape = boxes.shape\n",
    "    if not boxes_shape:\n",
    "        return\n",
    "    if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
    "        raise ValueError('Input must be of size [N, 4]')\n",
    "    for i in range(boxes_shape[0]):\n",
    "        draw_bounding_box_on_image(image, boxes[i, 1], boxes[i, 0], boxes[i, 3],\n",
    "                                 boxes[i, 2], color[i], thickness)\n",
    "\n",
    "\n",
    "def draw_bounding_boxes_on_image_array(image, boxes, color=[], thickness=5):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes on image (numpy array).\n",
    "    \n",
    "    Args:\n",
    "      image: a numpy array object.\n",
    "      boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
    "             The coordinates are in normalized format between [0, 1].\n",
    "      color: color to draw bounding box. Default is red.\n",
    "      thickness: line thickness. Default value is 4.\n",
    "      display_str_list_list: a list of strings for each bounding box.\n",
    "    \n",
    "    Raises:\n",
    "      ValueError: if boxes is not a [N, 4] array\n",
    "    \"\"\"\n",
    "\n",
    "    draw_bounding_boxes_on_image(image, boxes, color, thickness)\n",
    "  \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USx9tRBF8hWy"
   },
   "source": [
    "<a name=\"1-2\"></a>\n",
    "### 1.2 Data and Predictions Utilities\n",
    "\n",
    "We've given you some helper functions and code that are used to visualize the data and the model's predictions.\n",
    "\n",
    "- `display_digits_with_boxes`: This displays a row of \"digit\" images along with the model's predictions for each image.\n",
    "- `plot_metrics`: This plots a given metric (like loss) as it changes over multiple epochs of training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwJ4rZ1d_7ql"
   },
   "outputs": [],
   "source": [
    "# Matplotlib config\n",
    "plt.rc('image', cmap='gray')\n",
    "plt.rc('grid', linewidth=0)\n",
    "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
    "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
    "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
    "plt.rc('text', color='a8151a')\n",
    "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
    "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
    "\n",
    "\n",
    "# utility to display a row of digits with their predictions\n",
    "def display_digits_with_boxes(images, pred_bboxes, bboxes, iou, title, bboxes_normalized=False):\n",
    "\n",
    "    n = len(images)\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 4))\n",
    "    plt.title(title)\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "  \n",
    "    for i in range(n):\n",
    "      ax = fig.add_subplot(1, 10, i+1)\n",
    "      bboxes_to_plot = []\n",
    "      if (len(pred_bboxes) > i):\n",
    "        bbox = pred_bboxes[i]\n",
    "        bbox = [bbox[0] * images[i].shape[1], bbox[1] * images[i].shape[0], bbox[2] * images[i].shape[1], bbox[3] * images[i].shape[0]]\n",
    "        bboxes_to_plot.append(bbox)\n",
    "    \n",
    "      if (len(bboxes) > i):\n",
    "        bbox = bboxes[i]\n",
    "        if bboxes_normalized == True:\n",
    "          bbox = [bbox[0] * images[i].shape[1],bbox[1] * images[i].shape[0], bbox[2] * images[i].shape[1], bbox[3] * images[i].shape[0] ]\n",
    "        bboxes_to_plot.append(bbox)\n",
    "\n",
    "      img_to_draw = draw_bounding_boxes_on_image_array(image=images[i], boxes=np.asarray(bboxes_to_plot), color=[(255,0,0), (0, 255, 0)])\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "    \n",
    "      plt.imshow(img_to_draw)\n",
    "\n",
    "      if len(iou) > i :\n",
    "        color = \"black\"\n",
    "        if (iou[i][0] < iou_threshold):\n",
    "          color = \"red\"\n",
    "        ax.text(0.2, -0.3, \"iou: %s\" %(iou[i][0]), color=color, transform=ax.transAxes)\n",
    "        \n",
    "        \n",
    "# utility to display training and validation curves\n",
    "def plot_metrics(metric_name, title, ylim=5):\n",
    "    plt.title(title)\n",
    "    plt.ylim(0,ylim)\n",
    "    plt.plot(history.history[metric_name],color='blue',label=metric_name)\n",
    "    plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVkc7nzg-WUy"
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2. Preprocess and Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEEyTpmNxS0A"
   },
   "outputs": [],
   "source": [
    "def read_image_tfds(image, bbox):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    shape = tf.shape(image)\n",
    "\n",
    "    factor_x = tf.cast(shape[1], tf.float32)\n",
    "    factor_y = tf.cast(shape[0], tf.float32)\n",
    "\n",
    "    image = tf.image.resize(image, (224, 224,))\n",
    "\n",
    "    image = image/127.5\n",
    "    image -= 1\n",
    "\n",
    "    bbox_list = [bbox[0] / factor_x , \n",
    "                 bbox[1] / factor_y, \n",
    "                 bbox[2] / factor_x , \n",
    "                 bbox[3] / factor_y]\n",
    "    \n",
    "    return image, bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f10wa31DyeQ4"
   },
   "outputs": [],
   "source": [
    "def read_image_with_shape(image, bbox):\n",
    "    original_image = image\n",
    "    \n",
    "    image, bbox_list = read_image_tfds(image, bbox)\n",
    "    \n",
    "    return original_image, image, bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsQo9vvhyoKb"
   },
   "outputs": [],
   "source": [
    "def read_image_tfds_with_original_bbox(data):\n",
    "    image = data[\"image\"]\n",
    "    bbox = data[\"bbox\"]\n",
    "\n",
    "    shape = tf.shape(image)\n",
    "    factor_x = tf.cast(shape[1], tf.float32) \n",
    "    factor_y = tf.cast(shape[0], tf.float32)\n",
    "\n",
    "    bbox_list = [bbox[1] * factor_x , \n",
    "                 bbox[0] * factor_y, \n",
    "                 bbox[3] * factor_x, \n",
    "                 bbox[2] * factor_y]\n",
    "    return image, bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CF-luxkJyzIA"
   },
   "outputs": [],
   "source": [
    "def dataset_to_numpy_util(dataset, batch_size=0, N=0):\n",
    "\n",
    "    # eager execution: loop through datasets normally\n",
    "    take_dataset = dataset.shuffle(1024)\n",
    "\n",
    "    if batch_size > 0:\n",
    "        take_dataset = take_dataset.batch(batch_size)\n",
    "  \n",
    "    if N > 0:\n",
    "        take_dataset = take_dataset.take(N)\n",
    "  \n",
    "    if tf.executing_eagerly():\n",
    "        ds_images, ds_bboxes = [], []\n",
    "        for images, bboxes in take_dataset:\n",
    "            ds_images.append(images.numpy())\n",
    "            ds_bboxes.append(bboxes.numpy())\n",
    "        \n",
    "    return (np.array(ds_images), np.array(ds_bboxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZE8dgyPC1_6m"
   },
   "outputs": [],
   "source": [
    "def dataset_to_numpy_with_original_bboxes_util(dataset, batch_size=0, N=0):\n",
    "\n",
    "    normalized_dataset = dataset.map(read_image_with_shape)\n",
    "    if batch_size > 0:\n",
    "        normalized_dataset = normalized_dataset.batch(batch_size)\n",
    "  \n",
    "    if N > 0:\n",
    "        normalized_dataset = normalized_dataset.take(N)\n",
    "\n",
    "    if tf.executing_eagerly():\n",
    "        ds_original_images, ds_images, ds_bboxes = [], [], []\n",
    "        \n",
    "    for original_images, images, bboxes in normalized_dataset:\n",
    "        ds_images.append(images.numpy())\n",
    "        ds_bboxes.append(bboxes.numpy())\n",
    "        ds_original_images.append(original_images.numpy())\n",
    "\n",
    "    return np.array(ds_original_images), np.array(ds_images), np.array(ds_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HW_AyCNIKXR6"
   },
   "outputs": [],
   "source": [
    "def get_visualization_training_dataset():      \n",
    "    dataset, info = tfds.load(\"caltech_birds2010\", split=\"train\", with_info=True, data_dir=data_dir, download=False)\n",
    "    print(info)\n",
    "    visualization_training_dataset = dataset.map(read_image_tfds_with_original_bbox, \n",
    "                                                 num_parallel_calls=16)\n",
    "    return visualization_training_dataset\n",
    "\n",
    "\n",
    "visualization_training_dataset = get_visualization_training_dataset()\n",
    "\n",
    "\n",
    "(visualization_training_images, visualization_training_bboxes) = dataset_to_numpy_util(visualization_training_dataset, N=10)\n",
    "display_digits_with_boxes(np.array(visualization_training_images), np.array([]), np.array(visualization_training_bboxes), np.array([]), \"training images and their bboxes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qCuoUtYKXR6"
   },
   "source": [
    "Visualize the **validation** images and their bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLGiEyK_KXR6"
   },
   "outputs": [],
   "source": [
    "def get_visualization_validation_dataset():\n",
    "    dataset = tfds.load(\"caltech_birds2010\", split=\"test\", data_dir=data_dir, download=False)\n",
    "    visualization_validation_dataset = dataset.map(read_image_tfds_with_original_bbox, num_parallel_calls=16)\n",
    "    return visualization_validation_dataset\n",
    "\n",
    "\n",
    "visualization_validation_dataset = get_visualization_validation_dataset()\n",
    "\n",
    "(visualization_validation_images, visualization_validation_bboxes) = dataset_to_numpy_util(visualization_validation_dataset, N=10)\n",
    "display_digits_with_boxes(np.array(visualization_validation_images), np.array([]), np.array(visualization_validation_bboxes), np.array([]), \"validation images and their bboxes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5shayI_tzdq0"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "def get_training_dataset(dataset):\n",
    "    dataset = dataset.map(read_image_tfds, num_parallel_calls=16)\n",
    "    dataset = dataset.shuffle(512, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(-1) \n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(dataset):\n",
    "    dataset = dataset.map(read_image_tfds, num_parallel_calls=16)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.repeat()\n",
    "    return dataset\n",
    "\n",
    "training_dataset = get_training_dataset(visualization_training_dataset)\n",
    "validation_dataset = get_validation_dataset(visualization_validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8nHWWkS_eeZ"
   },
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3. Define the Network\n",
    "\n",
    "Bounding box prediction is treated as a \"regression\" task, in that you want the model to output numerical values.\n",
    "\n",
    "- You will be performing transfer learning with **MobileNet V2**.  The model architecture is available in TensorFlow Keras.\n",
    "- You'll also use pretrained `'imagenet'` weights as a starting point for further training.  These weights are also readily available \n",
    "- You will choose to retrain all layers of **MobileNet V2** along with the final classification layers.\n",
    "\n",
    "**Note:** For the following exercises, please use the TensorFlow Keras Functional API (as opposed to the Sequential API)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csyBuMZReYON"
   },
   "source": [
    "<a name='ex-01'></a>\n",
    "### Exercise 1\n",
    "\n",
    "Please build a feature extractor using MobileNetV2.\n",
    "\n",
    "- First, create an instance of the mobilenet version 2 model\n",
    "  - Please check out the documentation for [MobileNetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2)\n",
    "  - Set the following parameters:\n",
    "    - input_shape: (height, width, channel): input images have height and width of 224 by 224, and have red, green and blue channels.\n",
    "    - include_top: you do not want to keep the \"top\" fully connected layer, since you will customize your model for the current task.\n",
    "    - weights: Use the pre-trained 'imagenet' weights.\n",
    "  \n",
    "- Next, make the feature extractor for your specific inputs by passing the `inputs` into your mobilenet model.\n",
    "    - For example, if you created a model object called `some_model` and have inputs stored in `x`, you'd invoke the model and pass in your inputs like this: `some_model(x)` to get the feature extractor for your given inputs `x`.\n",
    "\n",
    "**Note**: please use mobilenet_v2 and not mobile_net or mobile_net_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DFecRhe0Pqc"
   },
   "outputs": [],
   "source": [
    "def feature_extractor(inputs):\n",
    "    \n",
    "    # Create a mobilenet version 2 model object\n",
    "    mobilenet_model = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(224, 224, 3),\n",
    "                                                                     include_top=False, \n",
    "                                                                     weights='imagenet')\n",
    "    \n",
    "\n",
    "    # pass the inputs into this modle object to get a feature extractor for these inputs\n",
    "    feature_extractor = mobilenet_model(inputs)\n",
    "        \n",
    "    # return the feature_extractor\n",
    "    return feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ufMK9Qy0VPM"
   },
   "source": [
    "<a name='ex-02'></a>\n",
    "### Exercise 2\n",
    "\n",
    "Next, you'll define the dense layers to be used by your model.\n",
    "\n",
    "You'll be using the following layers\n",
    "- [GlobalAveragePooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D): pools the `features`.\n",
    "- [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten): flattens the pooled layer.\n",
    "- [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense): Add two dense layers:\n",
    "    - A dense layer with 1024 neurons and a relu activation.\n",
    "    - A dense layer following that with 512 neurons and a relu activation.\n",
    "    \n",
    "**Note**: Remember, please build the model using the Functional API syntax (as opposed to the Sequential API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0njchQxB0b4Q"
   },
   "outputs": [],
   "source": [
    "def dense_layers(features):\n",
    "\n",
    "    # global average pooling 2d layer\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(features)     \n",
    "    \n",
    "    # flatten layer\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "    # 1024 Dense layer, with relu\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "    \n",
    "    # 512 Dense layer, with relu\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7ARvWYw0sje"
   },
   "source": [
    "<a name='ex-03'></a>\n",
    "### Exercise 3\n",
    "\n",
    "\n",
    "Now you'll define a layer that outputs the bounding box predictions. \n",
    "- You'll use a [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer.\n",
    "- Remember that you have _4 units_ in the output layer, corresponding to (xmin, ymin, xmax, ymax).\n",
    "- The prediction layer follows the previous dense layer, which is passed into this function as the variable `x`.\n",
    "- For grading purposes, please set the `name` parameter of this Dense layer to be `bounding_box'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdsD0-Jl07zW"
   },
   "outputs": [],
   "source": [
    "def bounding_box_regression(x):\n",
    "    \n",
    "    # Dense layer named `bounding_box`\n",
    "    bounding_box_regression_output = tf.keras.layers.Dense(4, name='bounding_box')(x)\n",
    "\n",
    "    return bounding_box_regression_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELxJoKqu1OnM"
   },
   "source": [
    "<a name='ex-04'></a>\n",
    "### Exercise 4\n",
    "\n",
    "Now, you'll use those functions that you have just defined above to construct the model.\n",
    "- feature_extractor(inputs)\n",
    "- dense_layers(features)\n",
    "- bounding_box_regression(x)\n",
    "\n",
    "Then you'll define the model object using [Model](https://www.tensorflow.org/s/results?q=Model).  Set the two parameters:\n",
    "- inputs\n",
    "- outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wn9O9c7I1XRJ"
   },
   "outputs": [],
   "source": [
    "def final_model(inputs):\n",
    "\n",
    "    # features\n",
    "    feature_cnn = feature_extractor(inputs)\n",
    "\n",
    "    # dense layers\n",
    "    last_dense_layer = dense_layers(feature_cnn) \n",
    "\n",
    "    # bounding box\n",
    "    bounding_box_output = bounding_box_regression(last_dense_layer)\n",
    "    \n",
    "    # define the TensorFlow Keras model using the inputs and outputs to your model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=bounding_box_output)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNWQP3dn1ftJ"
   },
   "source": [
    "<a name='ex-05'></a>\n",
    "### Exercise 5\n",
    "\n",
    "Define the input layer, define the model, and then compile the model. \n",
    "- inputs: define an [Input](https://www.tensorflow.org/api_docs/python/tf/keras/Input) layer\n",
    "  - Set the `shape` parameter.  Check your definition of `feature_extractor` to see the expected dimensions of the input image.\n",
    "- model: use the `final_model` function that you just defined to create the model.\n",
    "- compile the model: Check the [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) documentation for how to compile the model.\n",
    "  - Set the `optimizer` parameter to Stochastic Gradient Descent using [SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD)\n",
    "    - When using SGD, set the `momentum` to 0.9 and keep the default learning rate.\n",
    "  - Set the loss function of SGD to mean squared error (see the SGD documentation for an example of how to choose mean squared error loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C67ZmsTe1n9m"
   },
   "outputs": [],
   "source": [
    "def define_and_compile_model():\n",
    "\n",
    "    # define the input layer\n",
    "    inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
    "    \n",
    "    # create the model\n",
    "    model = final_model(inputs)\n",
    "    \n",
    "    # compile your model\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(momentum=0.90), \n",
    "                  loss=tf.keras.losses.mean_squared_error,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPtBf83B1zZ3"
   },
   "source": [
    "Run the cell below to define your model and print the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56y8UNFQIVwj"
   },
   "outputs": [],
   "source": [
    "# define your model\n",
    "model = define_and_compile_model()\n",
    "# print model layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtVVYVlvKXR7"
   },
   "source": [
    "<a name='4'></a>\n",
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuhDh8ao8VyB"
   },
   "source": [
    "<a name='4.1'></a>\n",
    "### 4.1 Prepare to Train the Model\n",
    "\n",
    "You'll fit the model here, but first you'll set some of the parameters that go into fitting the model.\n",
    "\n",
    "- EPOCHS: You'll train the model for 50 epochs\n",
    "- BATCH_SIZE: Set the `BATCH_SIZE` to an appropriate value. You can look at the ungraded labs from this week for some examples.\n",
    "- length_of_training_dataset: this is the number of training examples.  You can find this value by getting the length of `visualization_training_dataset`.\n",
    "  - Note: You won't be able to get the length of the object `training_dataset`. (You'll get an error message).\n",
    "- length_of_validation_dataset: this is the number of validation examples.  You can find this value by getting the length of `visualization_validation_dataset`.\n",
    "  - Note: You won't be able to get the length of the object `validation_dataset`.\n",
    "- steps_per_epoch: This is the number of steps it will take to process all of the training data.  \n",
    "  - If the number of training examples is not evenly divisible by the batch size, there will be one last batch that is not the full batch size.\n",
    "  - Try to calculate the number steps it would take to train all the full batches plus one more batch containing the remaining training examples. There are a couples ways you can calculate this.\n",
    "    - You can use regular division `/` and import `math` to use `math.ceil()` [Python math module docs](https://docs.python.org/3/library/math.html)\n",
    "    - Alternatively, you can use `//` for integer division, `%` to check for a remainder after integer division, and an `if` statement.\n",
    "  \n",
    "- validation_steps: This is the number of steps it will take to process all of the validation data.  You can use similar calculations that you did for the step_per_epoch, but for the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgTGU_j-KXR7"
   },
   "source": [
    "<a name='ex-06'></a>\n",
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoIY6xQ_KXR7"
   },
   "outputs": [],
   "source": [
    "# You'll train 50 epochs\n",
    "EPOCHS = 50\n",
    "\n",
    "# Choose a batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Get the length of the training set\n",
    "length_of_training_dataset = len(visualization_training_dataset)\n",
    "\n",
    "# Get the length of the validation set\n",
    "length_of_validation_dataset = len(visualization_validation_dataset)\n",
    "\n",
    "# Get the steps per epoch (may be a few lines of code)\n",
    "steps_per_epoch = length_of_training_dataset // BATCH_SIZE\n",
    "if steps_per_epoch % BATCH_SIZE > 0:\n",
    "    steps_per_epoch += 1\n",
    "\n",
    "# get the validation steps (per epoch) (may be a few lines of code)\n",
    "validation_steps = length_of_validation_dataset//BATCH_SIZE\n",
    "if length_of_validation_dataset % BATCH_SIZE > 0:\n",
    "    validation_steps += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWCj8CYtSQpY"
   },
   "source": [
    "<a name='4.2'></a>\n",
    "### 4.2 Fit the model to the data\n",
    "\n",
    "\n",
    "Check out the parameters that you can set to fit the [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).  Please set the following parameters.\n",
    "- x: this can be a tuple of both the features and labels, as is the case here when using a tf.Data dataset.\n",
    "  - Please use the variable returned from `get_training_dataset()`.\n",
    "  - Note, don't set the `y` parameter when the `x` is already set to both the features and labels.\n",
    "- steps_per_epoch: the number of steps to train in order to train on all examples in the training dataset.\n",
    "- validation_data: this is a tuple of both the features and labels of the validation set.\n",
    "  - Please use the variable returned from `get_validation_dataset()`\n",
    "- validation_steps: teh number of steps to go through the validation set, batch by batch.\n",
    "- epochs: the number of epochs.\n",
    "\n",
    "If all goes well your model's training will start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTL8VVAaKXR7"
   },
   "source": [
    "<a name='ex-07'></a>\n",
    "### Exercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTwH_P-ZJ_xx"
   },
   "outputs": [],
   "source": [
    "# Fit the model, setting the parameters noted in the instructions above.\n",
    "history = model.fit(training_dataset,\n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    validation_data=validation_dataset, \n",
    "                    validation_steps=validation_steps, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aBzmycIsO8w"
   },
   "source": [
    "<a name='5'></a>\n",
    "## 5. Validate the Model\n",
    "\n",
    "<a name='5-1'></a>\n",
    "### 5.1 Loss\n",
    "\n",
    "You can now evaluate your trained model's performance by checking its loss value on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWbkUql5sAok"
   },
   "outputs": [],
   "source": [
    "loss = model.evaluate(validation_dataset, steps=validation_steps)\n",
    "print(\"Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gjtus2EK0-hm"
   },
   "source": [
    "<a name='5-2'></a>\n",
    "### 5.2 Save your Model for Grading\n",
    "\n",
    "When you have trained your model and are satisfied with your validation loss, please you save your model so that you can upload it to the Coursera classroom for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Cvv-GgvE3V4"
   },
   "outputs": [],
   "source": [
    "# Please save your model\n",
    "model.save(\"birds.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CW2AAdkRsOMP"
   },
   "outputs": [],
   "source": [
    "# And download it using this shortcut or from the \"Files\" panel to the left\n",
    "from google.colab import files\n",
    "\n",
    "files.download(\"birds.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7E81sgUsUC4"
   },
   "source": [
    "<a name='5-3'></a>\n",
    "### 5.3 Plot Loss Function\n",
    "\n",
    "You can also plot the loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cz-b8TxU6EDj"
   },
   "outputs": [],
   "source": [
    "plot_metrics(\"loss\", \"Bounding Box Loss\", ylim=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G7KFVX9sXJt"
   },
   "source": [
    "<a name='5-4'></a>\n",
    "### 5.4 Evaluate performance using IoU\n",
    "\n",
    "You can see how well your model predicts bounding boxes on the validation set by calculating the Intersection-over-union (IoU) score for each image.\n",
    "\n",
    "- You'll find the IoU calculation implemented for you.\n",
    "- Predict on the validation set of images.\n",
    "- Apply the `intersection_over_union` on these predicted bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFqJxt3_VrCm"
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(pred_box, true_box):\n",
    "\n",
    "    xmin_pred, ymin_pred, xmax_pred, ymax_pred =  np.split(pred_box, 4, axis = 1)\n",
    "    xmin_true, ymin_true, xmax_true, ymax_true = np.split(true_box, 4, axis = 1)\n",
    "\n",
    "    #Calculate coordinates of overlap area between boxes\n",
    "    xmin_overlap = np.maximum(xmin_pred, xmin_true)\n",
    "    xmax_overlap = np.minimum(xmax_pred, xmax_true)\n",
    "    ymin_overlap = np.maximum(ymin_pred, ymin_true)\n",
    "    ymax_overlap = np.minimum(ymax_pred, ymax_true)\n",
    "\n",
    "    #Calculates area of true and predicted boxes\n",
    "    pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
    "    true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
    "\n",
    "    #Calculates overlap area and union area.\n",
    "    overlap_area = np.maximum((xmax_overlap - xmin_overlap),0)  * np.maximum((ymax_overlap - ymin_overlap), 0)\n",
    "    union_area = (pred_box_area + true_box_area) - overlap_area\n",
    "\n",
    "    # Defines a smoothing factor to prevent division by 0\n",
    "    smoothing_factor = 1e-10\n",
    "\n",
    "    #Updates iou score\n",
    "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
    "\n",
    "    return iou\n",
    "\n",
    "#Makes predictions\n",
    "original_images, normalized_images, normalized_bboxes = dataset_to_numpy_with_original_bboxes_util(visualization_validation_dataset, N=500)\n",
    "predicted_bboxes = model.predict(normalized_images, batch_size=32)\n",
    "\n",
    "\n",
    "#Calculates IOU and reports true positives and false positives based on IOU threshold\n",
    "iou = intersection_over_union(predicted_bboxes, normalized_bboxes)\n",
    "iou_threshold = 0.5\n",
    "\n",
    "print(\"Number of predictions where iou > threshold(%s): %s\" % (iou_threshold, (iou >= iou_threshold).sum()))\n",
    "print(\"Number of predictions where iou < threshold(%s): %s\" % (iou_threshold, (iou < iou_threshold).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jFVovcUUVs1"
   },
   "source": [
    "<a name='6'></a>\n",
    "## 6. Visualize Predictions\n",
    "\n",
    "Lastly, you'll plot the predicted and ground truth bounding boxes for a random set of images and visually see how well you did!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bR9Bb4uCwTyw"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "indexes = np.random.choice(len(predicted_bboxes), size=n)\n",
    "\n",
    "iou_to_draw = iou[indexes]\n",
    "norm_to_draw = original_images[indexes]\n",
    "display_digits_with_boxes(original_images[indexes], predicted_bboxes[indexes], normalized_bboxes[indexes], iou[indexes], \"True and Predicted values\", bboxes_normalized=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
